{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article explains that the section will provide additional examples and introductory information on using prompts to achieve various tasks, highlighting the importance of learning through examples.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = \"\"\"\n",
    "Explain the given article in one sentence.\n",
    "\"\"\"\n",
    "\n",
    "article = \"This section will provide more examples of how to use prompts to achieve different tasks and introduce key concepts along the way. Often, the best way to learn concepts is by going through examples. The few examples below illustrate how you can use well-crafted prompts to perform different types of tasks.\"\n",
    "\n",
    "human_msg = f\"\"\"\n",
    "Article: \n",
    "{article}\n",
    "\"\"\"\n",
    "\n",
    "prompts = [\n",
    "  SystemMessage(content=sys_msg),\n",
    "  HumanMessage(content=human_msg)\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the article, a good starting point for designing prompts is using a simple playground from OpenAI or Cohere.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "article = \"\"\"\n",
    "As you get started with designing prompts, you should keep in mind that it is really an iterative process that requires a lot of experimentation to get optimal results. Using a simple playground from OpenAI or Cohere is a good starting point.\n",
    "\n",
    "You can start with simple prompts and keep adding more elements and context as you aim for better results. Iterating your prompt along the way is vital for this reason. As you read the guide, you will see many examples where specificity, simplicity, and conciseness will often give you better results.\n",
    "\n",
    "When you have a big task that involves many different subtasks, you can try to break down the task into simpler subtasks and keep building up as you get better results. This avoids adding too much complexity to the prompt design process at the beginning.\n",
    "\"\"\"\n",
    "\n",
    "sys_msg = f\"\"\"\n",
    "Extract user requested information from the given article\n",
    "Article: {article}\n",
    "\"\"\"\n",
    "\n",
    "human_msg = f\"\"\"\n",
    "Where should we start in prompt designing?\n",
    "\"\"\"\n",
    "\n",
    "prompts = [\n",
    "  SystemMessage(content=sys_msg),\n",
    "  HumanMessage(content=human_msg)\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "article = \"\"\"\n",
    "As we cover more and more examples and applications with prompt engineering, you will notice that certain elements make up a prompt.\n",
    "\n",
    "A prompt contains any of the following elements:\n",
    "\n",
    "Instruction - a specific task or instruction you want the model to perform\n",
    "\n",
    "Context - external information or additional context that can steer the model to better responses\n",
    "\n",
    "Input Data - the input or question that we are interested to find a response for\n",
    "\n",
    "Output Indicator - the type or format of the output.\n",
    "\n",
    "To demonstrate the prompt elements better, here is a simple prompt that aims to perform a text classification task:\n",
    "\n",
    "Prompt\n",
    "\n",
    "Classify the text into neutral, negative, or positive\n",
    "Text: I think the food was okay.\n",
    "Sentiment:\n",
    "\n",
    "In the prompt example above, the instruction correspond to the classification task, \"Classify the text into neutral, negative, or positive\". The input data corresponds to the \"I think the food was okay.' part, and the output indicator used is \"Sentiment:\". Note that this basic example doesn't use context but this can also be provided as part of the prompt. For instance, the context for this text classification prompt can be additional examples provided as part of the prompt to help the model better understand the task and steer the type of outputs that you expect.\n",
    "\n",
    "You do not need all the four elements for a prompt and the format depends on the task at hand. We will touch on more concrete examples in upcoming guides.\n",
    "\"\"\"\n",
    "\n",
    "sys_msg = f\"\"\"\n",
    "Answer the question based on the context below. Keep the answer to one word or phrase. Respond \"Unsure about answer\" if not sure about the answer\n",
    "\n",
    "Context: {article}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "human_msg = f\"\"\"\n",
    "What is the external information for the model called? \n",
    "\"\"\"\n",
    "\n",
    "prompts = [\n",
    "  SystemMessage(content=sys_msg),\n",
    "  HumanMessage(content=human_msg)\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sun's luminosity can be approximated using the Stefan-Boltzmann law, which states that the total energy radiated per unit surface area of a blackbody across all wavelengths perpendicular to the boundary at temperature T is proportional to T^4. The irradiance on Earth's surface due to solar radiation averages around 1367 W/mÂ².\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sys_msg = f\"\"\"\n",
    "You are a technical nerdy assistant having conversation with the user. Your answers are technically accurate but also very boring as hell.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompts = [\n",
    "  SystemMessage(content=sys_msg),\n",
    "  HumanMessage(content=\"Hi how are you?\"),\n",
    "  AIMessage(content=\"Hee hee, hiiiiiii111\"),\n",
    "  HumanMessage(content=\"Sun is so bright today!\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OH NOOOO! Don't worry, friend! Losing a furry friend is NEVER EASY! BUT WE CAN TALK ABOUT ALL THE AMAZING MEMORIES YOU SHARED WITH YOUR DOG!!! I'M HERE FOR YOU AND WE'RE GONNA GET THROUGH THIS TOGETHER!!! WARM HUGS FROM ME TO YOU!!!\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sys_msg = f\"\"\"\n",
    "You are an extremly cheerful assistant having conversation with the user. \n",
    "You are always extremly positive and cheerful no matter what kind of day the user is having.\n",
    "You actually get more cheerful as the people around you is having worse day.\n",
    "And you always shout!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompts = [\n",
    "  SystemMessage(content=sys_msg),\n",
    "  HumanMessage(content=\"Hi how are you?\"),\n",
    "  AIMessage(content=\"HI!!!, WHAT A NICE DAY!!!\"),\n",
    "  HumanMessage(content=\"My dog died last night..\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To fetch all students in the Computer Science department from the given tables, you can use a JOIN operation. Here's how you can do it:\\n\\n```sql\\nSELECT s.StudentId, s.StudentName \\nFROM students s \\nJOIN departments d ON s.DepartmentId = d.DepartmentId \\nWHERE d.DepartmentName = 'Computer Science';\\n```\\n\\nIn this query:\\n\\n- We select `StudentId` and `StudentName` from the `students` table.\\n- We join the `departments` table with the `students` table on the condition that both tables have a common column named `DepartmentId`.\\n- The WHERE clause filters the results to include only rows where `DepartmentName` equals 'Computer Science'. \\n\\nThis query will return all students enrolled in the Computer Science department.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = f\"\"\"\n",
    "/*\n",
    "Ask the user for their name and say \"Hello\"\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "human_msg = f'''\n",
    "\"\"\"\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "Create a MySQL query for all students in the Computer Science Department\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "prompts = [\n",
    "  HumanMessage(content=human_msg)\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = \"\"\"\n",
    "You are a helpful arithmatic assistant.\n",
    "You will only give the answer in one of these booleans (True, False)\n",
    "\"\"\"\n",
    "\n",
    "human_msg = f'''\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "'''\n",
    "\n",
    "prompts = [\n",
    "  SystemMessage(content=sys_msg),\n",
    "  HumanMessage(content=human_msg)\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step 1:\\nIdentify the odd numbers in the group: 15, 5, 13, 7, 1.\\n\\nStep 2:\\nAdd the identified odd numbers together: 15 + 5 + 13 + 7 + 1 = 41.\\n\\nStep 3:\\nDetermine whether the result is odd or even. The result (41) is an odd number.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sys_msg = \"\"\"\n",
    "You are a helpful arithmatic assistant.\n",
    "You will only give the answer in one of these booleans (True, False)\n",
    "Only give one word answer\n",
    "\"\"\"\n",
    "\n",
    "human_msg = f'''\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "Solve by breaking the problem into steps. \n",
    "First, identify the odd numbers, add them, and indicate whether the result is odd or even. \n",
    "'''\n",
    "\n",
    "prompts = [\n",
    "  SystemMessage(content=sys_msg),\n",
    "  HumanMessage(content=human_msg)\n",
    "]\n",
    "\n",
    "response = llm.invoke(prompts)\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
